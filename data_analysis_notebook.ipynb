{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import traceback\n",
    "import spanlib as sl\n",
    "import sqlalchemy as sa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mpl_toolkits.mplot3d as Axes3D\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy import signal\n",
    "from scipy.linalg import hankel\n",
    "from sklearn.metrics import mutual_info_score, mean_squared_error\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data from SQL\n",
    "## Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_array(data):\n",
    "    \"\"\"Interprets json data as numpy ndarray.\"\"\"\n",
    "    \n",
    "    return np.array(json.loads(data))\n",
    "\n",
    "def import_data(db, table_name=\"10_op_point_test\"):\n",
    "    \"\"\"\n",
    "    Import data from database into arrays.\n",
    "    \n",
    "    Args:\n",
    "        db (database): database object from which to pull the data\n",
    "        table_name (string, default=\"10_op_point_test\"): name of table from which to pull the data\n",
    "    \n",
    "    Returns:\n",
    "        dict containing table data as numpy arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    data = db.execute(\"SELECT * FROM {}\".format(table_name))\n",
    "    data.fetchone()  # skip the first initialization row\n",
    "    key_list = data.keys()\n",
    "    print(\"Number of rows: {}\".format(data.rowcount-1))  # take off a row for initialization\n",
    "    \n",
    "    # Initilize the data dictionary\n",
    "    row = data.fetchone()\n",
    "    data_dict = {}\n",
    "    data_dict[key_list[0]] = np.array(row[key_list[0]])\n",
    "    data_dict[key_list[1]] = get_array(row[key_list[1]])\n",
    "    data_dict[key_list[2]] = get_array(row[key_list[2]])\n",
    "    data_dict[key_list[3]] = np.array(row[key_list[3]])\n",
    "    data_dict[key_list[4]] = np.array(datetime.strptime(row[key_list[4]], \"%Y-%m-%d@%H:%M:%S.%f\"))\n",
    "    data_dict[key_list[5]] = np.array(row[key_list[5]])\n",
    "    data_dict[key_list[6]] = get_array(row[key_list[6]])\n",
    "    data_dict[key_list[7]] = get_array(row[key_list[7]])\n",
    "    \n",
    "    try:\n",
    "        for index in range(data.rowcount-2):  # two rows used already\n",
    "            row = data.fetchone()\n",
    "            data_dict[key_list[0]] = np.append(data_dict[key_list[0]], row[key_list[0]])  # atmospheric pressure, psi\n",
    "            data_dict[key_list[1]] = np.vstack([data_dict[key_list[1]], get_array(row[key_list[1]])])  # desired flow voltage\n",
    "            data_dict[key_list[2]] = np.vstack([data_dict[key_list[2]], get_array(row[key_list[2]])])  # current flow voltage\n",
    "            data_dict[key_list[3]] = np.append(data_dict[key_list[3]], row[key_list[3]])  # flame status\n",
    "            data_dict[key_list[4]] = np.append(data_dict[key_list[4]], datetime.strptime(row[key_list[4]], \"%Y-%m-%d@%H:%M:%S.%f\"))  # date time stamp\n",
    "            data_dict[key_list[5]] = np.append(data_dict[key_list[5]], row[key_list[5]])  # static pressure, psi\n",
    "            data_dict[key_list[6]] = np.vstack([data_dict[key_list[6]], get_array(row[key_list[6]])])  # temperature readings, C\n",
    "            data_dict[key_list[7]] = np.append(data_dict[key_list[7]], get_array(row[key_list[7]]), axis=1)  # dynamic pressure measurements\n",
    "    except:\n",
    "        print(\"Error at index {}\".format(index))\n",
    "        traceback.print_exc()\n",
    "    db.close()\n",
    "    data_dict[\"time\"] = np.array([datetime.timestamp(dt) - datetime.timestamp(data_dict[\"dateTimeStamp\"][0]) for dt in data_dict[\"dateTimeStamp\"]])\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Establish mySQL database connection\n",
    "engine = sa.create_engine(\"mysql+pymysql://root:admin@localhost/mysql?charset=utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data = db.execute(\"SELECT * FROM 10_op_point_test\")\n",
    "data = import_data(db)\n",
    "if not db.closed:\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step = 1/10e3  # microphone sample period, sec\n",
    "mic_list = (\"Ambient\", \"Mic 0\", \"Mic 1\", \"Mic 2\", \"Mic 3\")  # for setting the legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells were used in testing and developing the data import helper function. They can be made runable again by selecting them in command mode and pressing `command-M+Y`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "data = db.execute(\"SELECT * FROM 10_op_point_test\")\n",
    "data.fetchone()  # skip the first initialization row\n",
    "row1 = data.fetchone()\n",
    "row2 = data.fetchone()\n",
    "if not db.closed:\n",
    "    db.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 2-D arrays\n",
    "data1 = get_array(row1[\"dynamicP\"])\n",
    "data2 = get_array(row2[\"dynamicP\"])\n",
    "print(data1.shape)\n",
    "combined = np.append(data1, data2, axis=1)\n",
    "print(combined.shape)\n",
    "\n",
    "# 1-D arrays\n",
    "data1 = get_array(row1[\"temperature\"])\n",
    "data2 = get_array(row2[\"temperature\"])\n",
    "print(data1.shape)\n",
    "combined = np.vstack([data1, data2])\n",
    "print(combined.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Create time vector. Note that this has been moved to the helper function, and does not need to be executed separately anymore.\n",
    "data[\"time\"] = np.array([datetime.timestamp(dt) - datetime.timestamp(data[\"dateTimeStamp\"][0]) for dt in data[\"dateTimeStamp\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check that all keys are present, and that values are formatted how we want\n",
    "print(data.keys())\n",
    "print(len(data[\"atmosphericP\"]))\n",
    "print(data[\"temperature\"][0:8])\n",
    "print(data[\"dynamicP\"][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot temperature data\n",
    "plt.plot(data[\"time\"], data[\"temperature\"])\n",
    "plt.xlabel(\"Time (sec)\")\n",
    "plt.ylabel(\"Temperature (C)\")\n",
    "plt.title(\"Measured Temperature vs. Time\")\n",
    "plt.legend([\"TC0\", \"TC1\", \"TC2\", \"TC3\", \"Flow\"], bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot operating point data\n",
    "plt.subplot(211)\n",
    "plt.plot(data[\"time\"], data[\"opPointDes\"])\n",
    "plt.ylabel(\"Desired (V)\")\n",
    "plt.title(\"Mass Flow Controller Voltage vs. Time\")\n",
    "plt.legend([\"air\", \"pilot\", \"middle\", \"outer\"], bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0)\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(data[\"time\"], data[\"opPointAct\"])\n",
    "plt.xlabel(\"Time (sec)\")\n",
    "plt.ylabel(\"Actual (V)\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequency analysis\n",
    "* Start by plotting the power spectra for the entire data set, just for fun.\n",
    "* Next we want to maybe look at windows in the data set: maybe 5-10 samples at a time (250-500 data points). Play with it.\n",
    "* Throw in some singular spectrum analysis\n",
    "* Other ideas...?\n",
    "\n",
    "## Start with plots of whole time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the time series for each microphone, just for fun.\n",
    "plt.plot(np.arange(0, step*data[\"dynamicP\"].shape[1], step), data[\"dynamicP\"].T)\n",
    "plt.xlabel(\"Time (sec)\")\n",
    "plt.ylabel(\"Reading (V)\")\n",
    "plt.title(\"Microphone Reading Time Series\")\n",
    "plt.legend(mic_list, bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot power spectra for dynamic pressure measurements for the entire experiment (not too useful)\n",
    "dyn_P_fft = np.fft.rfft(data[\"dynamicP\"], norm=\"ortho\")\n",
    "print(dyn_P_fft.shape)\n",
    "freq = np.fft.rfftfreq(data[\"dynamicP\"][0].size, step)\n",
    "plt.plot(freq, np.abs(dyn_P_fft.T)**2)  # power spectrum in dB\n",
    "# plt.plot(freq, np.angle(dyn_P_fft.T))  # phase spectrum\n",
    "plt.legend(mic_list)\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Power (V$^2$)\")\n",
    "plt.title(\"Dynamic Pressure Measurement Power Spectra\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Make an STFT waterfall plot\n",
    "This section attempts to make a waterfall plot generated by sliding a short-time fourier transform (STFT) across my time series. I got the idea from the [Kevin's Projects blog][1]\n",
    "\n",
    "[1]: https://kevinsprojects.wordpress.com/2014/12/13/short-time-fourier-transform-using-python-and-numpy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_stft(data, fft_size, fs, overlap_fac=0.5):\n",
    "    \"\"\"\n",
    "    Generates a short-time fourier transform waterfall matrix by performing FFT\n",
    "    of the input data over windows of fft_size length, overlapping by a factor\n",
    "    of overlap_fac until the end of the data set. The resulting matrix contains\n",
    "    the decibel power indexed by frequency and time.\n",
    "    \n",
    "    Args:\n",
    "        data (float array): long time series data to be analyzed\n",
    "        fft_size (int): number of samples used for each FFT window\n",
    "        fs (float): sample rate, Hz\n",
    "        overlap_fac (float, default=0.5): amount by which to overlap adjacent windows\n",
    "    \n",
    "    Returns:\n",
    "        array: Decibel power indexed by frequency and time\n",
    "    \"\"\"\n",
    "    \n",
    "    hop_size = np.int32(np.floor(fft_size * (1-overlap_fac)))\n",
    "    pad_end_size = fft_size  # the last segment can overlap the end of the data array by no more than one window size\n",
    "    total_segments = np.int32(np.ceil(len(data) / np.float32(hop_size)))\n",
    "    t_max = len(data) / np.float32(fs)\n",
    "    \n",
    "    window = np.hanning(fft_size)  # our half cosine window\n",
    "    inner_pad = np.zeros(fft_size)  # the zeros which will be used to double each segment size\n",
    " \n",
    "    proc = np.concatenate((data, np.zeros(pad_end_size)))  # the data to process\n",
    "    result = np.empty((total_segments, fft_size), dtype=np.float32)    # space to hold the result\n",
    " \n",
    "    for i in range(total_segments):  # for each segment\n",
    "        current_hop = hop_size * i  # figure out the current segment offset\n",
    "        segment = proc[current_hop:current_hop+fft_size]  # get the current segment\n",
    "        windowed = segment * window  # multiply by the half cosine function\n",
    "        padded = np.append(windowed, inner_pad)  # add 0s to double the length of the data\n",
    "        spectrum = np.fft.rfft(padded, norm=\"ortho\")  # take the Fourier Transform and scale by the number of samples\n",
    "        autopower = np.abs(spectrum)**2  # find the autopower spectrum\n",
    "        result[i, :] = autopower[:fft_size]  # append to the results array\n",
    " \n",
    "    result = 20*np.log10(result)  # scale to db\n",
    "    return (np.clip(result, -40, 200), t_max)  # clip values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do the STFT\n",
    "fft_size = 2500  # chunks of data over which I want to take my FFT\n",
    "fs = 1/step  # sample rate, Hz\n",
    "overlap_fac = 0.5  # amount by which to overlap windows. Chosen so that amplitude doesn't get all wonky\n",
    "result = []\n",
    "time = []\n",
    "for index in range(5):\n",
    "    res, t = do_stft(data[\"dynamicP\"][index], fft_size, fs, overlap_fac)\n",
    "    result.append(res)\n",
    "    time.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot the waterfall\n",
    "for res, t, mic in zip(result, time, mic_list):\n",
    "    plt.figure(figsize=(12,4), dpi=80)\n",
    "    img = plt.imshow(res, origin='lower', cmap='magma', interpolation='nearest', aspect='auto', extent=[0,fft_size,0,t])\n",
    "    cbar = plt.colorbar(img)\n",
    "    cbar.set_label(\"Magnitude, dB\")\n",
    "    plt.xticks(np.arange(0, 2501, 250))\n",
    "    plt.grid(b=True, axis=\"x\", color=\"w\", linestyle=\":\", linewidth=1)\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Time (sec)\")\n",
    "    plt.title(\"{} power spectrum waterfall plot in dB\".format(mic))\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Cross-correlation\n",
    "Basically take a short sample from an upstream microphone and slide it over the readings from a downstream microphone for the next short period of time, looking for any time delays between the two in which they strongly correlate. Ideally we can find strong correlation between microphones at a certain delay, and knowing the distance between our microphones, that delay gives us a good indication of our flow velocity. We may also be able to use this to start to look at traveling waves in the flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CC_waterfall(x, y, num_samp=1000, overlap_fac=0.5):\n",
    "    \"\"\"\n",
    "    Generate a 2D data set of the cross correlation between two different microphone samples where\n",
    "    each row represents cross correlation between a single sample from the first microphone and a\n",
    "    delayed sample with variable delay from the second microphone. Each column represents a new\n",
    "    sample from the first microphone. Overlap between first microphone samples are given by\n",
    "    overlap_fac.\n",
    "    \n",
    "    Args:\n",
    "        x (float array): first (upstream) microphone\n",
    "        y (float array): second (downstream) microphone over which we slide first mic sample\n",
    "        num_samp (int, default=1000): number of samples to compare, i.e. window size\n",
    "        overlap_fac (float, default=0.5): amount by which to overlap sample windows on first\n",
    "            data set. overlap_fac in [0, 1)\n",
    "    \n",
    "    Returns:\n",
    "        array: 2D array of cross correlation indexed by location (time) through the first\n",
    "            data set, and by time delay.\n",
    "    \"\"\"\n",
    "    \n",
    "    hop_size = np.int(np.floor(num_samp * (1-overlap_fac)))  # size of hops through first distribution, x\n",
    "    total_segments = np.int(np.floor((len(x) - 2*num_samp)/np.float(hop_size)))  # number of hops to end of x\n",
    "    result = np.empty((total_segments, num_samp+1))\n",
    "    start = 0  # moving start location for x\n",
    "    try:\n",
    "        for seg in range(total_segments):\n",
    "            stationary = y[start:start+2*num_samp]\n",
    "            cc = np.correlate(stationary, x[start:start+num_samp], mode=\"valid\")\n",
    "            result[seg,:] = cc\n",
    "            start += np.int(num_samp*(1-overlap_fac))  # move window across x\n",
    "    except:\n",
    "        print(\"Error at segment {}/{}, jump {}/{}\".format(seg, total_segments, num, num_jumps))\n",
    "        traceback.print_exc()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id0 = 1  # index of mic from which we take a short sample (upstream mic)\n",
    "id1 = 4  # index of mic over which we will slide our sample (downstream mic)\n",
    "num_samp = 500  # number of samples in second signal to slide across first\n",
    "start = int(30/step)  # start sample\n",
    "cross_corr = np.correlate(data[\"dynamicP\"][id1][start:start+2*num_samp], data[\"dynamicP\"][id0][start:start+num_samp], mode=\"valid\")\n",
    "delay = np.arange(0, cross_corr.size*step, step)\n",
    "print(cross_corr.shape)\n",
    "print(delay.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(delay[:len(cross_corr)], cross_corr)\n",
    "plt.xlabel(\"Offset time, sec\")\n",
    "plt.ylabel(\"Cross-correlation, V$^2$\")\n",
    "plt.title(\"Cross-correlation of {} and {}\".format(mic_list[id0], mic_list[id1]))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a cross-corrleation waterfall\n",
    "CC_data = CC_waterfall(data[\"dynamicP\"][id0], data[\"dynamicP\"][id1], num_samp, overlap_fac=0)\n",
    "print(CC_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4), dpi=80)\n",
    "img = plt.imshow(CC_data, origin='lower', cmap='magma', interpolation='nearest', aspect='auto', extent=[0,delay[-1],0,data[\"time\"][-1]])\n",
    "cbar = plt.colorbar(img)\n",
    "cbar.set_label(\"Cross correlation\")\n",
    "plt.xticks(np.arange(0, delay[-1], 0.02))\n",
    "plt.grid(b=True, axis=\"x\", color=\"w\", linestyle=\":\", linewidth=1)\n",
    "plt.xlabel(\"Delay (sec)\")\n",
    "plt.ylabel(\"Time (sec)\")\n",
    "plt.title(\"Cross correlation between {} and {}\".format(mic_list[id0], mic_list[id1]))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "peaks = signal.find_peaks_cwt(cross_corr, np.arange(10, 20))\n",
    "print(np.multiply(peaks, step))\n",
    "print(delay[np.where(cross_corr == max(cross_corr))])\n",
    "print(num_samp*step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mutual information\n",
    "Try to look at [mutual information][1] between microphones at different time lags. This might give a better indicator of the time delay between microphones than straight cross-correlation (above).\n",
    "\n",
    "Makes use of [sci-kit learn's `mutual_info_score`][2] for calculating mutual information.\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Mutual_information\n",
    "[2]: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mutual_info_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_MI(x, y, bins):\n",
    "    \"\"\"\n",
    "    Calculate the mutual information between two distributions.\n",
    "    \n",
    "    Args:\n",
    "        x (float array): true distribution\n",
    "        y (float array): predicted distribution\n",
    "        bins (int): number of bins to use in building a histogram of x and y\n",
    "    \n",
    "    Returns:\n",
    "        float: mutual information between x and y\n",
    "    \"\"\"\n",
    "    \n",
    "    c_xy = np.histogram2d(x, y, bins)[0]\n",
    "    mi = mutual_info_score(None, None, contingency=c_xy)\n",
    "    return mi\n",
    "\n",
    "def MI_waterfall(x, y, bins, num_samp=1000, jump=1, num_jumps=1000, overlap_fac=0.5):\n",
    "    \"\"\"\n",
    "    Generate a 2D data set of mutual information between two different microphone samples where\n",
    "    each row represents mutual information between a single sample from the first microphone and\n",
    "    a delayed sample with variable delay from the second microphone. Each column represents a\n",
    "    new sample from the first microphone. Overlap between first microphone samples are given by\n",
    "    overlap_fac.\n",
    "    \n",
    "    Args:\n",
    "        x (float array): true distribution\n",
    "        y (float array): predicted distribution\n",
    "        bins (int): number of bins to use in building a histogram of x and y\n",
    "        num_samp (int, default=1000): number of samples to compare, i.e. window size\n",
    "        jump (int, default=1): number of points to shift second window for each calculation\n",
    "        num_jumps (int, default=1000): number of times to shift second window\n",
    "        overlap_fac (float, default=0.5): amount by which to overlap sample windows on first\n",
    "            data set. overlap_fac in [0, 1)\n",
    "    \n",
    "    Returns:\n",
    "        array: 2D array of mutual information indexed by location (time) through the first\n",
    "            data set, and time delay.\n",
    "    \"\"\"\n",
    "    \n",
    "    hop_size = np.int(np.floor(num_samp * (1-overlap_fac)))  # size of hops through first distribution, x\n",
    "    total_segments = np.int(np.floor((len(x) - (jump*num_jumps+num_samp))/np.float(hop_size)))  # number of hops to end of x\n",
    "    result = np.empty((total_segments, num_jumps))\n",
    "    start = 0  # moving start location for x\n",
    "    progress = np.floor(total_segments/50)\n",
    "    print(\"Working \", end=\"\", sep=\"\")\n",
    "    try:\n",
    "        for seg in range(total_segments):\n",
    "            if seg % progress == 0:\n",
    "                print(\".\", end=\"\", sep=\"\", flush=True)\n",
    "            stationary = x[start:start+num_samp]\n",
    "            pos = start  # moving start location for y\n",
    "            mi = []\n",
    "            for num in range(num_jumps):\n",
    "                mi.append(calc_MI(stationary, y[pos:pos+num_samp], bins))\n",
    "                pos += jump\n",
    "            result[seg,:] = mi\n",
    "            start += np.int(num_samp*(1-overlap_fac))  # move window across x\n",
    "    except:\n",
    "        print(\"Error at segment {}/{}, jump {}/{}\".format(seg, total_segments, num, num_jumps))\n",
    "        traceback.print_exc()\n",
    "    print(\" done!\")\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id0 = 0  # index of mic from which we take a short sample (upstream mic)\n",
    "id1 = 0  # index of mic that we slide, trying to find match with upstream sample (downstream mic)\n",
    "num_samp = 2500  # size of chunks of signals that we are comparing\n",
    "start = int(30/step)  # start sample\n",
    "jump = 1  # number of samples to jump as we slide one distribution across the other\n",
    "num_jumps = 100  # number of times to jump\n",
    "bins = 10  # number of bins for MI histogram\n",
    "delay = np.arange(0, step*num_jumps*jump, step*jump)  # array of time delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mutual_info = []\n",
    "stationary = data[\"dynamicP\"][id0][start:start+num_samp]\n",
    "sliding = data[\"dynamicP\"][id1]\n",
    "for num in range(num_jumps):\n",
    "    pos = start+jump*num\n",
    "    mutual_info.append(calc_MI(stationary, sliding[pos:pos+num_samp], bins))\n",
    "mutual_info = np.array(mutual_info)\n",
    "delay = np.arange(0, mutual_info.size*step*jump, step*jump)\n",
    "print(mutual_info.shape)\n",
    "\n",
    "plt.plot(delay, mutual_info)\n",
    "plt.ylim(0, .5)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a mutual information waterfall plot\n",
    "MI_data = MI_waterfall(data[\"dynamicP\"][id0], data[\"dynamicP\"][id1], bins, num_samp, jump, num_jumps, overlap_fac=0.5)\n",
    "print(MI_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4), dpi=80)\n",
    "img = plt.imshow(20*np.log10(MI_data), origin='lower', cmap='magma', interpolation='nearest', aspect='auto', extent=[0,delay[-1],0,data[\"time\"][-1]])\n",
    "cbar = plt.colorbar(img)\n",
    "cbar.set_label(\"Mutual information, dB\")\n",
    "plt.xticks(np.arange(0, delay[-1], 0.002))\n",
    "plt.grid(b=True, axis=\"x\", color=\"w\", linestyle=\":\", linewidth=1)\n",
    "plt.xlabel(\"Delay (sec)\")\n",
    "plt.ylabel(\"Time (sec)\")\n",
    "plt.title(\"Mutual information between {} and {}\".format(mic_list[id0], mic_list[id1]))\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular Spectrum Analysis\n",
    "Singular spectrum analysis using the [spanlib library][1], which was a bit wonky to install, so I am not sure it is going to work at all, let alone that I will be able to get it to work meaningfully for me.\n",
    "\n",
    "[1]: http://relay.actimar.fr/~raynaud/spanlib/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_delay_mat(x, start=0, end=None, window=None):\n",
    "    \"\"\"\n",
    "    Build the trajectory matrix for use in singular spectrum analysis. The\n",
    "    trajectory matrix is made up of rows of length `window`, where each row\n",
    "    is the previous row right-shifted by one until the end of the time\n",
    "    series `x`.\n",
    "    \n",
    "    Args:\n",
    "        x (float array): time series that we are building into a trajectory\n",
    "            matrix\n",
    "        start (int, default=0): starting point for the embedding\n",
    "        end (int, default=len(x)): index of the last element used\n",
    "        window (int, default=len(x)/3): length of the rows of the trajectory\n",
    "            matrix.\n",
    "    \n",
    "    Returns:\n",
    "        array: trajectory matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    if end is None:\n",
    "        end = len(x)\n",
    "    if window is None:\n",
    "        window = np.int(np.floor((end-start)/3))\n",
    "    result = hankel(x[start:start+window], x[start+window:end+1])\n",
    "    return result\n",
    "        \n",
    "\n",
    "def delay_embed(x, start, N=1000, delay=1, num_delay=5):\n",
    "    \"\"\"\n",
    "    Create a delay-embedded matrix of N elements of the time series x by\n",
    "    stacking rows made up of N-element vectors from x where each row is\n",
    "    delayed by the specified amount from the previous row.\n",
    "    \n",
    "    Args:\n",
    "        x (float array): time series to be embedded\n",
    "        start (int): starting index in the time series\n",
    "    \n",
    "    Kwargs:\n",
    "        N (int, default=1000): number of samples used from time series\n",
    "        delay (int, default=1): number of samples to delay between each row\n",
    "        num_delays(int, default=5): number of times to apply delay (number of rows)\n",
    "    \n",
    "    Returns:\n",
    "        array: delay-embedded array of size (num_delay, N)\n",
    "    \"\"\"\n",
    "    \n",
    "    result = np.empty((num_delay, N))\n",
    "    for num in range(num_delay):\n",
    "        result[num,:] = x[start:start+N]\n",
    "        start += delay\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def reconstruct_mats(U, s, V, index_list):\n",
    "    \"\"\"\n",
    "    Build trajectory matrices from SVD matrices U and V, singular values s,\n",
    "    and an index list. The kth trajectory matrix comprising indicies i in I,\n",
    "    the trajectory matrix is given by:\n",
    "    \n",
    "    Tk = sum(sqrt(s[i])*(U[i] @ V[i].T))\n",
    "    \n",
    "    Args:\n",
    "        U (float array): left eigenvector matrix from SVD\n",
    "        s (float array): list of singular values from SVD\n",
    "        V (float array): right eigenvector matrix from SVD\n",
    "        index_list (float array): list of K indicies (can themselves be lists)\n",
    "            from which the K trajectory matrices are built.\n",
    "    \n",
    "    Returns:\n",
    "        float array: a K x L x L matrix of K trajectory matrices, whether L\n",
    "            is the lenght of the left and right eigenvectors (also the window\n",
    "            length used in the deconstruction step of SSA).\n",
    "    \"\"\"\n",
    "    \n",
    "    traj_mats = []\n",
    "    for inds in index_list:\n",
    "        if isinstance(inds, list):\n",
    "            Ti = []\n",
    "            for ind in inds:\n",
    "                Ti.append(np.sqrt(s[ind]) * (U[ind].reshape(len(U[ind]),1) @ V[ind].reshape(len(V[ind]),1).T))\n",
    "            traj_mats.append(np.sum(Ti, axis=0))\n",
    "        else:\n",
    "            traj_mats.append(np.sqrt(s[inds]) * (U[inds].reshape(len(U[inds]),1) @ V[inds].reshape(len(V[inds]),1).T))\n",
    "    return np.array(traj_mats)\n",
    "\n",
    "def diagonal_avg(traj_mats):\n",
    "    \"\"\"\n",
    "    Perform averaging over the anti-diagonals of the given matrices.\n",
    "    Return a list of vectors so averaged.\n",
    "    \n",
    "    Args:\n",
    "        traj_mats (np.ndarray): array of m LxK matrices over whose\n",
    "            anti-diagonals we are averaging\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: mx(K+L) matrix of diagonal-averaged arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    reconstructed = []\n",
    "    for mat in traj_mats:\n",
    "        mat = np.flipud(mat)  # need to flip for reverse diagonals\n",
    "        X = []\n",
    "        for offset in np.arange(-mat.shape[0]+1, mat.shape[1]):\n",
    "            diag = mat.diagonal(offset)\n",
    "            X.append(np.sum(diag)/diag.size)\n",
    "        reconstructed.append(X)\n",
    "    return np.array(reconstructed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single microphone SSA\n",
    "Attempt SSA of a single microphone with a delay embedding. Do this without spanlib to see whether results make sense with what spanlib is giving me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = np.int(np.ceil(10/step))\n",
    "N = 1000\n",
    "delay = 1\n",
    "num_delay = 10\n",
    "index = 2\n",
    "time = np.arange(start*step, (start+N)*step, step)[:N]\n",
    "# ssa_data = delay_embed(data[\"dynamicP\"][index], start, N, delay, num_delay).T\n",
    "ssa_data = data[\"dynamicP\"][index]\n",
    "delay_mat = build_delay_mat(ssa_data, start=0, end=1000, window=300)\n",
    "ssa_data = ssa_data[start:start+N]\n",
    "print(delay_mat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "U, s, V = np.linalg.svd(delay_mat, full_matrices=False)\n",
    "print(U.shape, s.shape, V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "big_SV = s[:30]\n",
    "plt.plot(big_SV/sum(s), \"o--\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Singular value spectrum of {} with delay embedding\".format(mic_list[index]))\n",
    "plt.xlabel(\"Singular value number\")\n",
    "plt.ylabel(\"Singular value contribution\")\n",
    "plt.xticks(np.arange(0, big_SV.size, np.ceil(big_SV.size/10)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "S = np.diag(s)\n",
    "np.allclose(delay_mat, np.dot(U, np.dot(S, V)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_list = [0, 1, [2,3], 4, 5, [6,7], 8, [9,10], 11, 12, 13, 14, [15,16,17], [18,19], 20, [21,22], [23,24,25], [26,27], [28,29]]\n",
    "for inds in index_list:\n",
    "    print(s[inds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traj_mats = reconstruct_mats(U, s, V, index_list)\n",
    "print(traj_mats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reconstructed = diagonal_avg(traj_mats)\n",
    "print(reconstructed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(np.sum(reconstructed, axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reproduced = np.sum(reconstructed, axis=0)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(time, ssa_data - reproduced, \"r\")\n",
    "plt.plot(time, ssa_data)\n",
    "plt.plot(time, reproduced)\n",
    "plt.legend([\"Error\", \"Actual\", \"Reproduced\"], bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0)\n",
    "plt.grid(True)\n",
    "plt.xlim(time[0], time[-1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reproduced.shape, ssa_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multichannel singular spectrum analysis\n",
    "MSSA with spanlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data[\"dynamicP\"].T[0:1000, :].shape\n",
    "start = np.int(np.ceil(10/step))\n",
    "N = 2000\n",
    "time = np.arange(start*step, (start+N)*step, step)[:N]\n",
    "ssa_data = data[\"dynamicP\"][:, start:start+N]\n",
    "span = sl.Analyzer(ssa_data.T, nmssa=14)\n",
    "print(ssa_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pc = span.mssa_pc()\n",
    "ev = span.mssa_ev()\n",
    "eof = span.mssa_eof()\n",
    "print(\"pc shape = {}\\nev shape = {}\\neof shape = {}\".format(pc.shape, ev.shape, eof.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(ev/np.sum(ev), \"bo--\")\n",
    "plt.title(\"Eigenvalue Spectrum\")\n",
    "plt.xlabel(\"Eigenvalue Index\")\n",
    "plt.ylabel(\"Eigenvalue\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "id0 = 0\n",
    "id1 = 1\n",
    "component = np.sqrt(ev[id0])*eof[id0] + np.sqrt(ev[id1])*eof[id1]\n",
    "plt.figure()\n",
    "plt.plot(component)\n",
    "plt.legend(mic_list, bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ec = span.mssa_ec(xeof=eof)\n",
    "rec = span.mssa_rec(xpc=pc)\n",
    "print(\"ec shape = {}\\nrec shape = {}\".format(ec.shape, rec.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index = 3\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(time, data[\"dynamicP\"][index, start:start+N])\n",
    "plt.plot(time, rec[:, index], linewidth=2)\n",
    "plt.plot(time, data[\"dynamicP\"][index, start:start+N] - rec[:, index], \"r\")\n",
    "plt.xlim([time[0], time[-1]])\n",
    "plt.title(\"Actual and SSA-Reproduced {} Signal\".format(mic_list[index]))\n",
    "plt.xlabel(\"Time, sec\")\n",
    "plt.ylabel(\"Reading, V\")\n",
    "plt.legend([\"Actual\", \"Reproduced\", \"Error\"], bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like this is at least doing what the algorithm is meant to do: pick out features of the time series and then reproduce the series, assuming that any features that do not make the final cut are noise. I probably need to run this with quite a few more features. Basically the feature number, or number of eigenvalues, corresponds to the number of time delays used for embedding to generate the trajectory matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change detection with SSA or MSSA\n",
    "I think I could use MSSA or just SSA for change detection, according to [Moskvina and Zhigljavsky 2003][1]. I'm sure I could find some more recent (and possibly better) applications along these lines, but the basic idea is that you use your SSA analysis as a base point and then compare a test embedding to this base to find some distance measure from the test to the base. If the distance is large enough, you can say that the underlying process that produced the time series has changed. This seems like a pretty offline-focused algorithm since SSA takes a while...\n",
    "\n",
    "1. Do SSA with L delays on the first N samples of data, giving essentially K = N-L+1 samples of length-K feature vectors.\n",
    "2. Do SSA on a later test series of length Q.\n",
    "3. Find some distance measure from the reproduction of the second series to the reproduction of the first series. I think we can actually find the distance measure direclty between the second series and the reproduction of the first series, without needing to do the second SSA maybe.\n",
    "4. If the distance is above some threshold, a change has occurred. Otherwise, no change.\n",
    "5. Move ahead one chunk. Find a new base series (maybe your previous test series?) and compare it with a new test series to look for changes.\n",
    "\n",
    "[1]: http://www.tandfonline.com/doi/abs/10.1081/SAC-120017494"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "start = np.int(np.ceil(10/step))\n",
    "N = 1000  # length of base sample\n",
    "Q = 1000  # length of test sample\n",
    "time = np.arange(start*step, (start+N)*step, step)[:N]\n",
    "base_data = data[\"dynamicP\"][:, start:start+N]\n",
    "test_data = data[\"dynamicP\"][:, start+N+1:start+N+1+Q]\n",
    "base = sl.Analyzer(base_data.T, nmssa=8)\n",
    "test = sl.Analyzer(test_data.T, nmssa=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proper orthogonal decomposition\n",
    "Basically we take the `n x m` matrix of `n` samples from `m` microphones and perform singular value decomposition on it. The system can then be reproced with a `k`-order model using the first `k` singular values and left and right eigenvectors from the SVD.\n",
    "\n",
    "The Wikipedia article on [principle component analysis][1] covers this.\n",
    "\n",
    "[1]: https://en.wikipedia.org/wiki/Principal_component_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Perform the SVD\n",
    "start = np.int(10/step)\n",
    "num_elem = 2000\n",
    "time = np.arange(start*step, (start+num_elem)*step, step)[:num_elem]\n",
    "POD_data = data[\"dynamicP\"].T[start:start+num_elem, :]\n",
    "U, s, V = np.linalg.svd(POD_data, full_matrices=False)\n",
    "U.shape, s.shape, V.shape, time.shape, POD_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(s, \"bo--\")\n",
    "plt.title(\"Singular value spectrum\")\n",
    "plt.xlabel(\"Singular value number\")\n",
    "plt.ylabel(\"Singular value magnitude\")\n",
    "plt.xticks(range(s.size))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test that we can reproduce the signal with our SVD\n",
    "S = np.diag(s)\n",
    "np.allclose(POD_data, np.dot(U, np.dot(S, V)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test how many components of the SVD are needed to accurately reproduce the signal\n",
    "mi = np.empty([s.size, s.size])\n",
    "mse = np.empty(mi.shape)\n",
    "for index in np.arange(s.size):\n",
    "    for num_comp in np.arange(s.size):\n",
    "        reproduced = np.dot(U[:, :num_comp], np.dot(np.diag(s[:num_comp]), V[:num_comp, :]))\n",
    "        hist_xy = np.histogram2d(POD_data[:,index], reproduced[:,index])[0]\n",
    "        mi[num_comp, index] = mutual_info_score(None, None, hist_xy)\n",
    "        mse[num_comp, index] = mean_squared_error(POD_data[:,index], reproduced[:,index])\n",
    "\n",
    "comps = np.arange(1, s.size+1)\n",
    "plt.figure()\n",
    "plt.plot(comps, mi)\n",
    "plt.grid(True)\n",
    "plt.title(\"Mutual information between actual and POD-reproduced signal\")\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"Mutual information\")\n",
    "plt.xticks(comps)\n",
    "plt.legend(mic_list, bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0)\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(comps, np.log10(mse))\n",
    "# plt.yscale(\"log\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Mean squared error between actual and POD-reproduced signal\")\n",
    "plt.xlabel(\"Number of components\")\n",
    "plt.ylabel(\"$\\log_{10}$(MSE)\")\n",
    "plt.xticks(comps)\n",
    "plt.legend(mic_list, bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_comp = 2\n",
    "index = 2\n",
    "reproduced = np.dot(U[:, :num_comp], np.dot(np.diag(s[:num_comp]), V[:num_comp, :]))\n",
    "\n",
    "plt.plot(time, POD_data[:, index])\n",
    "plt.plot(time, reproduced[:,index])\n",
    "plt.plot(time, POD_data[:, index] - reproduced[:,index])\n",
    "plt.xlim(time[0], time[-1])\n",
    "plt.title(\"Actual vs. reproduced signal for {}\".format(mic_list[index]))\n",
    "plt.xlabel(\"Time, sec\")\n",
    "plt.ylabel(\"Amplitude, V\")\n",
    "plt.legend([\"Actual\", \"Reproduced\", \"Error\"], bbox_to_anchor=(1.05, 1), loc=\"upper left\", borderaxespad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
