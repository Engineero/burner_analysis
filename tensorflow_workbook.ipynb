{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# import mpl_toolkits.mplot3d as Axes3D\n",
    "\n",
    "from database import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "\n",
    "Generate training, testing, and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 42331\n",
      "Keys: ['dynamicP', 'opPointDes', 'staticP', 'atmosphericP', 'flameStatus', 'temperature', 'opPointAct', 'dateTimeStamp']\n",
      "Loading data ................................................... done!\n",
      "Elapsed time: 42.98329856200144 sec\n"
     ]
    }
   ],
   "source": [
    "# Import data from database\n",
    "step = 1/10e3  # microphone sample period, sec\n",
    "mic_list = (\"Ambient\", \"Mic 0\", \"Mic 1\", \"Mic 2\", \"Mic 3\")  # for setting the legend\n",
    "fs = 1/step  # sample rate, Hz\n",
    "\n",
    "# Load data from the database\n",
    "# host = \"mysql.ecn.purdue.edu\"  # 128.46.154.164\n",
    "# user = \"op_point_test\"\n",
    "# database = \"op_point_test\"\n",
    "host = 'localhost'\n",
    "user = 'root'\n",
    "database = 'mysql'\n",
    "table_name = '100_op_point_test'\n",
    "password = 'admin'\n",
    "# with open(\"password.txt\", \"r\") as f:\n",
    "#   password = f.read().rstrip()\n",
    "eng = connect_to_db(host, user, password, database)\n",
    "tic = timeit.default_timer()\n",
    "data = import_data(eng, table_name)\n",
    "toc = timeit.default_timer()\n",
    "if eng.open:\n",
    "    eng.close()\n",
    "print(\"Elapsed time: {} sec\".format(toc-tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import processed data from pickle\n",
    "processed_data = []\n",
    "for mic in mic_list:\n",
    "    fname = os.path.join('.', 'Processed', 'short_fft_waterfall_{}.pickle'.format(mic))\n",
    "    processed_data.append(pickle.load(open(fname, 'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42332, 511), (42332, 501))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = []\n",
    "labelset = []\n",
    "for num in range(data['opPointAct'].shape[0]):\n",
    "    start = num*100\n",
    "    vec = np.concatenate((data['flameStatus'][num], data['opPointAct'][num], data['temperature'][num],\n",
    "                          data['staticP'][num],\n",
    "                          np.concatenate([row for row in data['dynamicP'][:, start:start+100]])), axis=0)\n",
    "    dataset.append(vec)\n",
    "    vec = np.concatenate((data['flameStatus'][num],\n",
    "                          np.concatenate([row['res'][num,:] for row in processed_data])), axis=0)\n",
    "    labelset.append(vec)\n",
    "dataset = np.array(dataset)\n",
    "labelset = np.array(labelset)\n",
    "dataset.shape, labelset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_size = dataset.shape[1]\n",
    "output_size = labelset.shape[1]\n",
    "num_samples = dataset.shape[0]\n",
    "batch_size = 64\n",
    "num_unrollings = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BatchGenerator(object):\n",
    "    def __init__(self, data, batch_size, num_unrollings):\n",
    "        self._data = data\n",
    "        self._num_samples = data.shape[0]\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._num_samples // batch_size\n",
    "        self._offset = 0\n",
    "        self._last_batch = self._next_batch()\n",
    "\n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = self._data[self._offset:self._offset+self._batch_size, :]\n",
    "        self._offset += self._batch_size\n",
    "        if self._num_samples - self._offset < self._batch_size:\n",
    "            self._offset = 0\n",
    "        return batch\n",
    "\n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_size = np.int(0.85*num_samples)\n",
    "valid_size = len(dataset) - train_size\n",
    "train_batches = BatchGenerator(dataset[:train_size,:], batch_size, num_unrollings)\n",
    "train_label_batches = BatchGenerator(labelset[:train_size,:], batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(dataset[train_size:,:], 1, 1)\n",
    "valid_label_batches = BatchGenerator(labelset[train_size:,:], 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM RNN Definition\n",
    "\n",
    "Define the long short-term memory (LSTM) recurrent neural network (RNN). Borrowing heavily from the Deep Learning Udacity course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64  #TODO tune this!\n",
    "num_steps = 1001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # Parameters\n",
    "    # Combined input, forget, memory cell, and output weights for input and previous state. Also biases.\n",
    "    ifcox = tf.Variable(tf.truncated_normal([input_size, 4*num_nodes], 0.0, 1/np.sqrt(input_size)))\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], 0.0, 1/np.sqrt(input_size)))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, output_size], 0.0, 1/np.sqrt(output_size)))\n",
    "    b = tf.Variable(tf.zeros([output_size]))\n",
    "    \n",
    "    # Create saver\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Definition of the cell computation\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        combined = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        input_gate = tf.sigmoid(combined[:, :num_nodes])\n",
    "        forget_gate = tf.sigmoid(combined[:, num_nodes:2*num_nodes])\n",
    "        update = combined[:, 2*num_nodes:3*num_nodes]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(combined[:, 3*num_nodes:])\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # Input data\n",
    "    train_data = list()\n",
    "    train_labels = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size, input_size]))\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, output_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_outputs = train_labels[1:]  # labels are shifted by one time step.\n",
    "    \n",
    "    # Unrolled LSTM loop\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "    \n",
    "    # Saving state across unrollings\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits,\n",
    "                tf.concat(0, train_outputs)))\n",
    "    \n",
    "    # Optimizer\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 2*num_steps//3, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, input_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "            saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output,\n",
    "            saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run\n",
    "\n",
    "Train the network on the dataset and observe the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized.\n",
      "Average loss at step 0: 6.200928688049316 learning rate: 10.0\n",
      "Minibatch perplexity: 493.20673794490546\n",
      "Average loss at step 100: 0.007128275972049441 learning rate: 10.0\n",
      "Minibatch perplexity: 1.0012317684903522\n",
      "Average loss at step 200: 0.0031575521343620494 learning rate: 10.0\n",
      "Minibatch perplexity: 1.0005054479820132\n",
      "Average loss at step 300: 0.0027715624694246797 learning rate: 10.0\n",
      "Minibatch perplexity: 1.0016356704979006\n",
      "Average loss at step 400: 0.0031859814579365776 learning rate: 10.0\n",
      "Minibatch perplexity: 1.001267044307595\n",
      "Average loss at step 500: 0.0027533012168714775 learning rate: 10.0\n",
      "Minibatch perplexity: 1.0173151158684255\n",
      "Average loss at step 600: 0.0029846351256128402 learning rate: 10.0\n",
      "Minibatch perplexity: 1.0011311792758943\n",
      "Average loss at step 700: 0.003126926047843881 learning rate: 1.0\n",
      "Minibatch perplexity: 1.0013175536929602\n",
      "Average loss at step 800: 0.0027722151251509787 learning rate: 1.0\n",
      "Minibatch perplexity: 1.0012982689702687\n",
      "Average loss at step 900: 0.003116587434196845 learning rate: 1.0\n",
      "Minibatch perplexity: 1.001307338888127\n",
      "Average loss at step 1000: 0.0025566860754042864 learning rate: 1.0\n",
      "Minibatch perplexity: 1.0012720127993744\n",
      "Validation set perplexity: 1.001238050091451\n",
      "Model saved in file: ./tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "summary_frequency = 100\n",
    "mean_loss = 0\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized.')\n",
    "    for step in range(num_steps):\n",
    "        # Generate next training batch\n",
    "        batches = train_batches.next()\n",
    "        labels = train_label_batches.next()\n",
    "        \n",
    "        # Generate feed_dict\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            feed_dict[train_labels[i]] = labels[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "                [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        \n",
    "        # Update to the user periodically\n",
    "        if step % summary_frequency == 0:\n",
    "            # Output some information about our training performance\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step {}: {} learning rate: {}'.format(step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            print('Minibatch perplexity: {}'.format(float(np.exp(logprob(predictions,\n",
    "                    np.concatenate(labels[1:], axis=0))))))\n",
    "\n",
    "    # Measure validation set perplexity.\n",
    "    reset_sample_state.run()\n",
    "    valid_logprob = 0\n",
    "    for num in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        l = valid_label_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, l[1])\n",
    "    print('Validation set perplexity: {}'.format(float(np.exp(valid_logprob / valid_size))))\n",
    "    \n",
    "    save_path = saver.save(session, './tmp/model.ckpt')\n",
    "    print('Model saved in file: {}'.format(save_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze\n",
    "\n",
    "Analyze the performance and see what we can do with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model restored.\n",
      "Logprob prediction accuracy: 1.0114345435720058\n",
      "Logprob prediction accuracy: 1.0017291618745077\n",
      "Logprob prediction accuracy: 1.001293345008001\n",
      "Logprob prediction accuracy: 1.0012435861506115\n",
      "Logprob prediction accuracy: 1.001236654929073\n",
      "Logprob prediction accuracy: 1.0012364158655769\n",
      "Logprob prediction accuracy: 1.0012364158655256\n",
      "Logprob prediction accuracy: 1.0012364158657951\n",
      "Logprob prediction accuracy: 1.0012364159959932\n",
      "Logprob prediction accuracy: 1.0012364158655291\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "with tf.Session(graph=graph) as session:\n",
    "    saver.restore(session, './tmp/model.ckpt')\n",
    "    print('Model restored.')\n",
    "    reset_sample_state.run()\n",
    "    valid_logprob = 0\n",
    "    for num in range(10):\n",
    "        b = valid_batches.next()\n",
    "        l = valid_label_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = logprob(predictions, l[1])\n",
    "        print('Logprob prediction accuracy: {}'.format(np.exp(valid_logprob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
